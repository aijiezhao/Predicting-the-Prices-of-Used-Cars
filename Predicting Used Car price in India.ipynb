{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Install packages and read data\nlibrary(pacman,tidyverse)\np_load(tidyverse,janitor, caret, glmnet, magrittr, \n       dummies,janitor,rpart.plot,gbm,\n       ggplot2,ggpubr, viridis, reshape2, \n       cowplot,ranger, pROC, plotROC, dplyr, RColorBrewer,\n       formattable)\ntotal_df<- read_csv(\"~/Downloads/train-data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":false},"cell_type":"code","source":"\n\n#Data cleaning\n#The reason is electric only have 2 oberservations.\ntotal_df<- subset(total_df, total_df$Fuel_Type != 'Electric')\n# Split the dataset for train and test dataset(20%)\ntest_df <- sample_frac(total_df,size=0.2)\ntrain_df<- setdiff(total_df, test_df)\ntotal_df<-rbind(train_df,test_df)\n\n# Preprocessing data (-New_Price vairbales, because it contains 86% missing value)\ntotal<- total_df %>% select(-Price,-X,-New_Price)\ntotal %>% clean_names()\ntotal %>% glimpse()\n\n# Data cleaning- categorical and numerical variables\n#Create new Age variable substitute year.\ntotal_df <- total %>% mutate(Age = 2020-Year) %>% select(-Year)\ntotal_df$Power<- as.numeric(total_df$Power)\n\n# Milage has two different units, so we need classify accordingly\n# kmpkg will only have values for CNG/LPG cars,\n# kmpl will only have values for Diesel/Petrol cars, then we can create two new variables.\ntotal_df %<>%\n  mutate(kmpkg = ifelse(Fuel_Type==\"CNG\" | Fuel_Type==\"LPG\", Mileage, 0)) %>%\n  mutate(kmpL = ifelse(Fuel_Type==\"Diesel\" | Fuel_Type==\"Petrol\", Mileage, 0)) %>%\n  select(-Mileage)\n\n#Select Name only use car brands, instead of car types\ntype <- substr(total_df$Name,1,4)\ntype\ntotal_df$Power<- as.numeric(total_df$Power)\nsummary(total_df$Power)\ntotal_df$Name <- type\ntotal_df <- dummy.data.frame(total_df)\n\n# deal missing value (Seats) & center scale standerlize\ntotal_m <- preProcess(\n  x = total_df,\n  method = c('medianImpute','center' , 'scale')\n) %>% predict(total_df)\n\n# dummy variables to deal with character variables\ntotal_d <- dummy.data.frame(total_m)\n\n# Split final clean data\ntrain_df1 <- head(total_d, 4814)\ntest_df1 <- tail(total_d, 1203)\nPrice <- train_df$Price\ntrain_df1 <- cbind(Price, train_df1)\n\n##set seed\nset.seed(19981994)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Model1: Lasso Regression\nlambdas = 10^seq(from =5, to = -2, length =100)\ntrain_lasso<- glmnet(\n  x = train_df1 %>% dplyr :: select(-Price) %>% as.matrix(),\n  y = train_df1$Price,\n  standardize = T,\n  alpha = 1,\n  lambda = lambdas\n)\n\n## Cross Validation\nlasso_cv <- cv.glmnet(\n  x = train_df1 %>% dplyr :: select(-Price) %>% as.matrix(),\n  y = train_df1$Price,\n  standardize = T,\n  alpha = 1,\n  lambda = lambdas,\n  ## How we make decisions based on number of folds\n  type.measure = \"mse\",\n  nfolds = 5\n)\n## Final model for lasso\nfinal_lasso <- glmnet(\n  x = train_df1 %>% dplyr :: select(-Price) %>% as.matrix(),\n  y = train_df1$Price,\n  standardize = T,\n  alpha = 1,\n  lambda = lasso_cv$lambda.min\n)\n\n## Graph to see the results\nlasso_cv1 <- train(\n  Price ~.,\n  data = train_df1,\n  method = 'glmnet',\n  trControl = trainControl(\"cv\",number = 5),\n  tuneGrid = expand.grid(alpha = 1, lambda = lambdas)\n)\n\n\nmin(lasso_cv1$results$RMSE)\n##5.653751\nlasso_cv$lambda.min\n##0.01\n\n## Using the coef function to check the lasso has selected\ncoef(final_lasso)\n\n## Make the predition with the lasso model\nlasso_prediction <-predict(\n  final_lasso,\n  type = \"response\",\n  s = lasso_cv$lambda.min,\n  newx = as.matrix(test_df1)\n)\n\ntest_df$lasso_prediction <- lasso_prediction\nlasso_cv1$results\nmean(abs(test_df$Price-test_df$lasso_prediction))\n##2.996764\nmin(lasso_cv1$results$RMSE)\n##5.653751\n##max Rsquared: 0.7519921\nmin(lasso_cv1$results$MAE)\n##3.054344\n\n\n##graph\nImp_l <- varImp(lasso_cv1)\nlasso_Imp <- data.frame(variable = names(train_df1 %>% select (-Price)), \n                        overall = Imp_l$importance$Overall)\nlasso_Imp <- lasso_Imp[order(lasso_Imp$overall, decreasing = TRUE),]\nlasso_Imp_head <- head(lasso_Imp,10)\n\nlasso_Imp_graph <- ggplot(lasso_Imp_head, aes(variable, y = overall,fill = overall))+coord_flip()+\n  geom_col()+\n  theme_light()+scale_fill_distiller(palette = 'Paired')\n\nlasso_Imp_graph\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Model2: Boosting\nboosting <- train(\n  Price~.,\n  data = train_df1,\n  method = 'gbm',\n  trControl = trainControl(\n    method = \"cv\",\n    number = 5\n  ),\n  tuneGrid = expand.grid(\n    \"n.trees\" = seq(25,200, by = 25),\n    'interaction.depth'=1:3,\n    'shrinkage'= c(.1,0.01,0.001),\n    'n.minobsinnode'=10\n  )\n)\n\n\n## Make prediction\nboosting_prediction<- predict(\n  boosting,\n  newdata = test_df1\n)\n\ntest_df$boosting_prediction <- boosting_prediction\nmean(abs(test_df$Price-test_df$boosting_prediction))\n##1.690242\nboosting$results <- boosting$results[order(boosting$results$RMSE),]\nboosting$results \nmin(boosting$results$RMSE)\n##3.860461\nmax(boosting$results$Rsquared)\n##0.8853001\nmin(boosting$results$MAE)\n##1.906654\n\n##graph\n\nImp_b <- varImp(boosting)\nboosting_Imp <- data.frame(variable = names(train_df1 %>% select (-Price)), \n                           overall = Imp_b$importance$Overall)\nboosting_Imp <- boosting_Imp[order(boosting_Imp$overall, decreasing = TRUE),]\nboosting_Imp_head <- head(boosting_Imp,10)\n\nboosting_Imp_graph <- ggplot(boosting_Imp_head, aes(variable, y = overall,fill = overall))+coord_flip()+\n  geom_col()+\n  theme_light()+scale_fill_distiller(palette = 'Paired')\n\nboosting_Imp_graph\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Model 3: Linear regression\nlinear_regression <- train(\n  Price ~.,\n  method = 'lm',\n  data = train_df1,\n  trControl = trainControl(method = \"cv\", number = 5),\n)\n\nlinear_regression$results\n## intercept     RMSE  Rsquared      MAE    RMSESD RsquaredSD     MAESD\n##1      TRUE 5.634924 0.7545236 3.118241 0.6078225 0.02667565 0.1415909\n\n## Make the prediction with linear regression model\nlm_prediction <- predict(linear_regression, newdata = test_df1 )\n\ntest_df$lm_prediction <- lm_prediction\nmean(abs(test_df$Price - test_df$lm_prediction))\n##3.01408\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Model 4: Random Forest\nrandom_forest <- train(\n  Price ~ .,\n  data = train_df1,\n  method = \"ranger\",\n  num.trees = 100,\n  trControl = trainControl(method = 'oob'),\n  tuneGrid = expand.grid(\n    'mtry' = 2:8,\n    'splitrule' = 'extratrees',\n    'min.node.size' = 1:20\n  )\n)\n\n\n## Make prediction\nrandomforest_prediction <- predict(\n  random_forest,\n  newdata = test_df1\n)\n\n## Check the error rate\ntest_df$randomforest_prediction <- randomforest_prediction\nmean(abs(test_df$Price - test_df$randomforest_prediction))\n##1.916815\nrandom_forest$results \nmin(random_forest$results$RMSE)\n##4.473427\nmax(random_forest$results$Rsquared)\n##0.8556015\nmin(random_forest$results$MAE)\n##2.058779","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Model5: Ridge Regression\nlambdas = 10^seq(from =5, to = -2, length =100)\ntrain_ridge<- glmnet(\n  x = train_df1 %>% dplyr :: select(-Price) %>% as.matrix(),\n  y = train_df1$Price,\n  standardize = T,\n  alpha = 0,\n  lambda = lambdas\n)\n\n## Cross Validation\nridge_cv <- cv.glmnet(\n  x = train_df1 %>% dplyr :: select(-Price) %>% as.matrix(),\n  y = train_df1$Price,\n  standardize = T,\n  alpha = 0,\n  lambda = lambdas,\n  ## How we make decisions based on number of folds\n  type.measure = \"mse\",\n  nfolds = 5\n)\n## Final model for lasso\nfinal_ridge <- glmnet(\n  x = train_df1 %>% dplyr :: select(-Price) %>% as.matrix(),\n  y = train_df1$Price,\n  standardize = T,\n  alpha = 0,\n  lambda = ridge_cv$lambda.min\n)\n\n## Graph to see the results\nridge_cv1 <- train(\n  Price ~.,\n  data = train_df1,\n  method = 'glmnet',\n  trControl = trainControl(\"cv\",number = 5),\n  tuneGrid = expand.grid(alpha = 0, lambda = lambdas)\n)\n\nmin(ridge_cv1$results$RMSE)\n##5.66239\nridge_cv$lambda.min\n##0.05094138\n\n## Using the coef function to check the lasso has selected\ncoef(final_ridge)\n\n## Make the predition with the ridge model\nridge_prediction <-predict(\n  final_ridge,\n  type = \"response\",\n  s = ridge_cv$lambda.min,\n  newx = as.matrix(test_df1)\n)\n\ntest_df$ridge_prediction <- ridge_prediction\nmean(abs(test_df$Price-test_df$ridge_prediction))\n##3.00211\nridge_cv1$results \nmin(ridge_cv1$results$RMSE)\n##5.66239\n##max ridge_cv1$results$Rsquared :0.7533918\n\nmin(ridge_cv1$results$MAE)\n##3.013093\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Make the table to report the final results\nmodel <- c('Lasso', 'Boosting', 'Linear regression','Random Forest','Ridge')\nTest_MAE <- c(2.996764,1.690242,3.01408,1.916815,3.00211)\nmin_RMSE <- c (5.653751,3.860461,5.634924,4.473427,5.6623)\nmax_Rsquared <- c(0.7519921,0.8853001,0.7545236,0.8556015,0.7533918)\nmin_MAE <- c(3.054344,1.906654,3.118241,2.058779,3.013093)\n\n\nFinal_result <- data.frame(model = model, Test_MAE = Test_MAE, min_RMSE=min_RMSE,\n                           max_Rsquared=max_Rsquared, min_MAE=min_MAE \n                           )\n\nformattable(Final_result, align =c(\"l\",\"c\",\"c\",\"c\",\"c\"), list(\n  `model` = formatter(\"span\", style = ~ style(color = \"grey\",font.weight = \"bold\")), \n  `Test_MAE`= color_tile(\"pink\", \"gray\"),\n  `min_RMSE`= color_tile(\"pink\", \"gray\"),\n  `max_Rsquared`= color_tile(\"gray\", \"pink\"),\n  `min_MAE`= color_tile(\"pink\", \"gray\")))\n\ngraph_results <- melt(Final_result)\nggplot (graph_results, aes(model, value, fill= variable))+\n  geom_bar(stat=\"identity\",position=\"dodge\")+theme_light()+coord_flip()+\n  scale_fill_viridis(discrete = T)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":4}